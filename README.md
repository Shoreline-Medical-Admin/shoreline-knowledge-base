# AWS Bedrock Knowledge Base Q&A Agent

[![CI](https://github.com/langchain-ai/new-langgraph-project/actions/workflows/unit-tests.yml/badge.svg)](https://github.com/langchain-ai/new-langgraph-project/actions/workflows/unit-tests.yml)
[![Integration Tests](https://github.com/langchain-ai/new-langgraph-project/actions/workflows/integration-tests.yml/badge.svg)](https://github.com/langchain-ai/new-langgraph-project/actions/workflows/integration-tests.yml)

This LangGraph agent retrieves information from AWS Bedrock Knowledge Base and generates answers to user questions. It uses a multi-node graph architecture with retrieval-augmented generation (RAG) to provide accurate, context-based responses.

<div align="center">
  <img src="./static/studio_ui.png" alt="Graph view in LangGraph studio UI" width="75%" />
</div>

## Features

- **Multiple Knowledge Base Support**: Query from Medical Guidelines and CMS Coding knowledge bases
- **AWS Bedrock Knowledge Base Integration**: Retrieves relevant documents from your knowledge bases
- **Retrieval-Augmented Generation**: Uses retrieved context to generate accurate answers
- **Source Attribution**: Shows which KB each source came from with relevance scores
- **Reasoning Transparency**: Displays confidence scores and reasoning process for each answer
- **Uncertainty Detection**: Identifies and warns about limited sources, outdated information, or conflicts
- **Configurable Parameters**: Customize model, temperature, and retrieval settings
- **Flexible Querying**: Choose to query medical KB, CMS KB, or both
- **Error Handling**: Graceful handling of AWS authentication and retrieval errors
- **LangGraph Studio Support**: Visual debugging and monitoring of the agent workflow

## Getting Started

<!--
Setup instruction auto-generated by `langgraph template lock`. DO NOT EDIT MANUALLY.
-->

<!--
End setup instructions
-->

1. Install dependencies, along with the [LangGraph CLI](https://langchain-ai.github.io/langgraph/concepts/langgraph_cli/), which will be used to run the server.

```bash
cd path/to/your/app
pip install -e . "langgraph-cli[inmem]"
```

2. Set up your AWS credentials and configuration. Create a `.env` file:

```bash
cp .env.example .env
```

Edit the `.env` file with your AWS and Bedrock configuration:

```text
# AWS Configuration (REQUIRED)
AWS_ACCESS_KEY_ID=your_access_key_here
AWS_SECRET_ACCESS_KEY=your_secret_key_here
AWS_DEFAULT_REGION=us-east-1

# Bedrock Knowledge Base Configuration
MEDICAL_GUIDELINES_KB_ID=VXMUOUXXCF
CMS_CODING_KB_ID=X1DCXMHW9T

# Which knowledge bases to query ("medical", "cms", or "both")
KNOWLEDGE_BASES=both

# Optional: Customize model settings
# Using Claude Sonnet 4 with US inference profile (latest)
BEDROCK_MODEL_ID=us.anthropic.claude-sonnet-4-20250514-v1:0
BEDROCK_MODEL_TEMPERATURE=0.3
BEDROCK_MAX_RESULTS=5
```

3. Start the LangGraph Server.

```shell
langgraph dev
```

For more information on getting started with LangGraph Server, [see here](https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/).

## Architecture

The agent uses a multi-node graph with the following flow:

1. **retrieve_documents**: Queries AWS Bedrock Knowledge Base for relevant documents and evaluates source quality
2. **generate_answer**: Uses retrieved context with Bedrock LLM to generate an answer, considering confidence levels
3. **format_response**: Formats the final response with source attribution and reasoning explanation

## Usage

Once the server is running, you can interact with the agent through LangGraph Studio or the API:

```python
# Example request - Query both knowledge bases
{
    "query": "What are the coding guidelines for diabetes management?",
    "configurable": {
        "medical_guidelines_kb_id": "VXMUOUXXCF",
        "cms_coding_kb_id": "X1DCXMHW9T",
        "knowledge_bases": "both",
        "aws_region": "us-east-1",
        "model_id": "us.anthropic.claude-sonnet-4-20250514-v1:0",
        "max_results": 5,
        "temperature": 0.3
    }
}

# Example - Query only medical guidelines
{
    "query": "What are the treatment protocols for hypertension?",
    "configurable": {
        "knowledge_bases": "medical"
    }
}

# Example - Query only CMS coding
{
    "query": "What is the CPT code for a routine physical exam?",
    "configurable": {
        "knowledge_bases": "cms"
    }
}
```

## Configuration Options

- **medical_guidelines_kb_id**: Medical Guidelines Knowledge Base ID
- **cms_coding_kb_id**: CMS Coding Knowledge Base ID  
- **knowledge_bases**: Which KBs to query - "medical", "cms", or "both" (default: both)
- **aws_region**: AWS region for Bedrock services (default: us-east-1)
- **model_id**: Bedrock model to use for generation (default: Claude Sonnet 4)
- **max_results**: Maximum number of documents to retrieve per KB (default: 5)
- **temperature**: Model temperature for response generation (default: 0.3)

## Reasoning Transparency

The agent now includes reasoning transparency features that help users understand how answers are generated:

### Confidence Scoring
- **ðŸŸ¢ High Confidence (80-100%)**: Multiple relevant sources with high scores
- **ðŸŸ¡ Medium Confidence (60-79%)**: Adequate sources but some limitations
- **ðŸŸ  Low Confidence (40-59%)**: Limited or less relevant sources
- **ðŸ”´ Very Low Confidence (<40%)**: Minimal sources or significant issues

### Uncertainty Detection
The agent automatically detects and warns about:
- **Limited Sources**: When fewer than 3 documents are found
- **Outdated Information**: Sources older than 2 years
- **Conflicting Sources**: Significant disagreement between documents
- **Low Relevance**: No highly relevant sources (score < 80%)

### Reasoning Process
Each response includes a collapsible "Show Reasoning Process" section that displays:
- Step-by-step reasoning with confidence impacts
- Source evaluation scores (relevance, recency, authority)
- Detailed explanations of any detected issues

### Disabling Reasoning
To disable reasoning transparency in a specific query:
```python
{
    "query": "Your question here",
    "show_reasoning": false
}
```

## Prerequisites

1. **AWS Account**: You need an AWS account with Bedrock access
2. **Knowledge Base**: Create a Bedrock Knowledge Base with your documents
3. **IAM Permissions**: Ensure your AWS credentials have permissions for:
   - `bedrock:InvokeModel`
   - `bedrock:Retrieve`

## Development

While iterating on your graph in LangGraph Studio, you can edit past state and rerun your app from previous states to debug specific nodes. Local changes will be automatically applied via hot reload.

Follow-up requests extend the same thread. You can create an entirely new thread, clearing previous history, using the `+` button in the top right.

For more advanced features and examples, refer to the [LangGraph documentation](https://langchain-ai.github.io/langgraph/). These resources can help you adapt this template for your specific use case and build more sophisticated conversational agents.

LangGraph Studio also integrates with [LangSmith](https://smith.langchain.com/) for more in-depth tracing and collaboration with teammates, allowing you to analyze and optimize your chatbot's performance.

<!--
Configuration auto-generated by `langgraph template lock`. DO NOT EDIT MANUALLY.
{
  "config_schemas": {
    "agent": {
      "type": "object",
      "properties": {}
    }
  }
}
-->
